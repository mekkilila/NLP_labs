{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab-5_NLP_ENSAE.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d2f261646c324ecf8c6022e669a6a1ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f4371a31a7ea4f58be714496614b3fc0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e494eb5b597b4e91b04fd4ef88e1683e","IPY_MODEL_26df476d328c43bda95d86a6b4baf1e8"]},"model_module_version":"1.5.0"},"f4371a31a7ea4f58be714496614b3fc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"e494eb5b597b4e91b04fd4ef88e1683e":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9394812d58b84021a84619f394a45ce9","_dom_classes":[],"description":"Downloading","_model_name":"IntProgressModel","bar_style":"success","max":445032417,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":445032417,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb722f53a92a4ad0b37f8d944189edc3"},"model_module_version":"1.5.0"},"26df476d328c43bda95d86a6b4baf1e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e1154638400847debef3ab7503bab3b4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 445M/445M [00:17&lt;00:00, 25.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61e502cc9fb04d85a7f96746ee5dffd9"},"model_module_version":"1.5.0"},"9394812d58b84021a84619f394a45ce9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"eb722f53a92a4ad0b37f8d944189edc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"e1154638400847debef3ab7503bab3b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"61e502cc9fb04d85a7f96746ee5dffd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"idqQXEkKsmKb"},"source":["# Machine Learning for NLP : lab session 5\n","\n","### Lectures takeaways (5 and 6)\n","\n","- Language Models \n","- The Transformer Architecture\n","- Use a Pretrained Language Model on specific tasks (focus on BERT)\n","\n","### Lab session outline \n","\n","1. Playing with BERT and the **transformer** library\n","  1. Experimenting with the CamemBERT language model\n","2. Fine-tuning BERT for task specific use cases \n","\n","### Resources : \n","\n","\n","- Library doc:                  https://huggingface.co/transformers/quickstart.html \n","\n","- ADAM Optimizer https://mlfromscratch.com/optimizers-explained/ \n","\n","- Transformer architecture: http://jalammar.github.io/illustrated-transformer/\n","- BERT:     https://arxiv.org/pdf/1810.04805.pdf\n","- CamemBERT: https://arxiv.org/pdf/1911.03894.pdf \n","\n","source : https://medium.com/towards-artificial-intelligence/cross-lingual-language-model-56a65dba9358\n","\n"," \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"chHe8ukMsIFw","outputId":"4ab0e0ee-1d36-4732-d446-8203d94c4159","colab":{"base_uri":"https://localhost:8080/","height":666}},"source":["  !pip install transformers\n","#!pip install torch torchvision"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n","\r\u001b[K     |▋                               | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 5.0MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 6.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 6.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 81kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 174kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 184kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 194kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 204kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 215kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 225kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 235kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 245kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 256kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 266kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 276kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 286kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 296kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 307kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 317kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 327kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 337kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 348kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 358kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 368kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 378kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 389kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 399kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 409kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 419kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 430kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 440kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 450kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 460kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 471kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 481kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 491kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 501kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 24.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.18)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 40.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 48.5MB/s \n","\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.18)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=ae97a6f8aea275e925a6410cdc7777962534853b409fcb6d1e164f91ccb4041c\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AiAa8AFfuD8T"},"source":["## About the Transformers library \n","\n","Transformers is an opinionated library built for NLP researchers seeking to use/study/extend large-scale transformers models.\n","\n","The library was designed with two strong goals in mind:\n","\n","- be as easy and fast to use as possible:\n","- povide state-of-the-art models with performances as close as possible to the original models\n","\n","cf. for more details https://huggingface.co/transformers/quickstart.html \n","\n","**In other words, <font color='red'> the Transformers library is currently one of the best open-source tools (if not the best) to experiment with the best NLP models** </font> \n","\n","\n","In the context of this course : the Transformers library will be a great tool to : \n","- play with **SOTA pretrained language models** (BERT and others)\n","- **fine-tune on specific tasks**\n","You can find the list of all available pretrained models in the library here : https://huggingface.co/transformers/pretrained_models.html \n","\n","In short, The Transformers library is a collection of wrappers built with Pytorch or Tensorflow that provides model loading, prediction, training, or fine-tuning. \n","\n","### What will we do with the Transformers library ?\n","\n","- We will load a pretrained language model for English: BERT \n","- We will visualize sentence embeddings\n","- We will fine-tune BERT for sentiment analysis\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E59ZFW8_Uzcl"},"source":["\n","\n","### What's a Masked language model again ? \n","\n","BERT is a Masked Language Model\n","\n","![Texte alternatif…](https://drive.google.com/uc?id=1d6TMVu6G8azV07wJN02igAWAVzClPdVH)\n","\n","... "]},{"cell_type":"markdown","metadata":{"id":"GNJ_sLgbCJg0"},"source":["### Loading the model \n","\n","#### 1- Tokenizer : \n","\n","As seen during the lectures, tokenization is a model specific stage. \n","In the context of Mask-Language Models tokenization work at the sub-word level (cf. Byte-Pair Encoding Lecture 6), we therefore need to load a model specific bpe-tokenizer. \n","\n","#### 2- Loading the pretrained weights \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"yWq_5sx_s8jz","outputId":"81f3e805-3ee6-460e-a336-b27406f4fdeb","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["### Loading a model \n","import pdb\n","import torch\n","import torch.nn as nn\n","from transformers import AutoModel, AutoTokenizer\n","from transformers import XLMRobertaForMaskedLM, XLMRobertaTokenizer\n","\n","## OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n","#import logging\n","#logging.basicConfig(level=logging.)\n","\n","MODEL_NAME = \"camembert-base\"\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","# Tokenize input\n","text = tokenizer.bos_token+\" Les feux de brousse qui sévissent depuis septembre en Australie, favorisés par des températures exceptionnelles, dépassent tous les records. \"+tokenizer.eos_token\n","tokenized_text = tokenizer.tokenize(text)\n","\n","# Mask a token that we will try to predict back with `BertForMaskedLM`\n","masked_index = 13 # e.g mask the word 'Australie' index by 13 \n","tokenized_text[masked_index] = tokenizer.mask_token\n","#assert tokenized_text == ['<s>', '▁Les', '▁feux', '▁de', '▁b', 'rousse', '▁qui', '▁s', 'év', 'issent', '▁depuis', '▁septembre', '▁en', '<mask>', ',', '▁favorisé', 's', '▁par', '▁des', '▁températures', '▁exceptionnelles', ',', '▁dépassent', '▁tous', '▁les', '▁records', '.', '</s>'], \"ERROR {}\".format(tokenized_text)\n","print(\"Input text is {}\".format(tokenized_text))\n","# Convert token to vocabulary indices\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","segments_ids = [0 for _ in indexed_tokens]\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])\n","tokens_tensor, segments_tensors"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input text is ['<s>', '▁Les', '▁feux', '▁de', '▁b', 'rousse', '▁qui', '▁s', 'év', 'issent', '▁depuis', '▁septembre', '▁en', '<mask>', ',', '▁favorisé', 's', '▁par', '▁des', '▁températures', '▁exceptionnelles', ',', '▁dépassent', '▁tous', '▁les', '▁records', '.', '</s>']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(tensor([[    5,    74,  7795,     8,  1011, 15380,    31,    52,  5632,  5999,\n","            176,   652,    22, 32004,     7, 21438,    10,    37,    20,  6350,\n","          15039,     7, 18851,   117,    19, 18588,     9,     6]]),\n"," tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0]]))"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"TvqR5FimNFkS"},"source":["Questions : \n","\n","1. Why is tokenization required ? Why do special token bos_token and eos_token are required ? \n","2. Why those tokens are special ? \n","3. Why some token start with ▁ symbol and some do not ? \n"]},{"cell_type":"markdown","metadata":{"id":"PiNwrajmBbzs"},"source":["### About GPU \n","\n","GPU provide much faster large matrix operations compare to CPU. \n","In Colab : go to _Modifier -> Paramètres du Notebook -> Accélérateur Matériel\"_\n","\n","\n","GPU computing is based on a _cuda backend_ . To put your model in the GPU of your computer, simply apply the following commands. \n","\n","NB : In order to perform GPU computing, all the involved tensors should be in cuda datatypes ! Put them all in the gpu with the .to('cuda') function\n"]},{"cell_type":"code","metadata":{"id":"4DZm1qR0lXkL","outputId":"e2c18ded-cb0a-4638-c7bf-881166f6c07a","colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["# to checkout the GPU activity (cf. Volatile GPU-Util %)\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed Mar 18 06:11:22 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.59       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    56W / 149W |    791MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6pI0frFxtT-O","outputId":"e36d2940-1fcf-45a5-9094-b4b259506e98","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["d2f261646c324ecf8c6022e669a6a1ff","f4371a31a7ea4f58be714496614b3fc0","e494eb5b597b4e91b04fd4ef88e1683e","26df476d328c43bda95d86a6b4baf1e8","9394812d58b84021a84619f394a45ce9","eb722f53a92a4ad0b37f8d944189edc3","e1154638400847debef3ab7503bab3b4","61e502cc9fb04d85a7f96746ee5dffd9"]}},"source":["# Load pre-trained model (weights)\n","\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","# Set the model in evaluation mode to deactivate the DropOut modules\n","# This is IMPORTANT to have reproducible results during evaluation!\n","model.eval()\n","\n","# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n","\n","# Predict hidden states features for each layer\n","with torch.no_grad():\n","    # See the models docstrings for the detail of the inputs\n","    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    # Transformers models always output tuples.\n","    # See the models docstrings for the detail of all the outputs\n","    # In our case, the first element is the hidden state of the last layer of the Bert model\n","    encoded_layers = outputs[0]\n","# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n","assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2f261646c324ecf8c6022e669a6a1ff","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=445032417, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"guNvFygCNz_P"},"source":["**Questions** \n","\n","4. Why are GPU helpful in some cases when we do Deep Learning for NLP ?\n","5. What are the constraints of GPU ? \n"]},{"cell_type":"markdown","metadata":{"id":"EapvuofWCa7L"},"source":["### Experimenting with BERT\n","\n","Now that we loaded the model and the tokenizer, let's experiment with it. \n","\n","This lab session is focused on BERT. BERT is a Masked-Language Model based \n","on the Transformer architecture. \n","\n","We first do some qualitative experiments with BERT. Let's first analyze BERT as a Language Model. Then we will use it to produce sentence embedding. \n","\n","We will first play with CamemBERT, the French version of BERT. We use the transformer MaskLM wrapper to perform language modelling with it. \n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uBz8Gs9QUSpv"},"source":["#### Language Modelling with **CamemBERT**"]},{"cell_type":"code","metadata":{"id":"LFSX3zJWHkpP","outputId":"b4ab71b7-aa95-493b-cf03-59dce47cba76","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Load pre-trained model (weights)\n","from transformers import AutoModelWithLMHead\n","\n","model = AutoModelWithLMHead.from_pretrained('camembert-base')\n","model.eval()\n","# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n","\n","# Predict all tokens\n","with torch.no_grad():\n","    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    predictions = outputs[0]\n","\n","predictions.size(), predictions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 28, 32005]),\n"," tensor([[[ 24.2970,  -4.9570,   7.4591,  ...,  -6.7556,  -3.8422,   1.5580],\n","          [  0.8888,  -5.1120,  20.0042,  ..., -10.3810,  -2.8260,   1.2345],\n","          [  0.7079,  -4.8593,   5.9372,  ...,  -2.2902,  -9.3024,  -4.2323],\n","          ...,\n","          [  3.1483,  -5.8761,   9.6652,  ...,   0.6660, -14.9198,  -1.6405],\n","          [  6.5931,  -9.3456,   6.4012,  ...,  -6.1560,  -7.7579,   1.6097],\n","          [  9.2209,  -6.0294,  27.4842,  ...,  -9.5553,  -6.6236,   2.0224]]],\n","        device='cuda:0'))"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"CRS1-ZanPHSh"},"source":["**Questions :**\n","\n","7. What each predictions.size() dimension correspond to ?\n","8. Fill the cell below to compute the mask language model prediction in our example  \n","9. Same question but with top-5 prediction "]},{"cell_type":"code","metadata":{"id":"2WR5pgXARvmC","outputId":"ae4f8f9c-4192-40cb-ac6d-941454fbd662","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Question 8 \n","#predicted_index =  # f(predictions[0, masked_index])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([11046, 22552,  2971,  6278,   184], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"qCm3KAlAK1j-"},"source":["def detokenized_text(tokenized_sequence, masked_index, special_char=\"▁\"):\n","  \"\"\"\n","  We reconstruct the original text and the prediction \n","  input: bpe index sequence\n","  return: \n","  \"\"\"\n","  detokenized_text = \"\"\n","  for ind, token in enumerate(tokenized_sequence):\n","    if ind==masked_index:\n","      detokenized_text+=\" **\"\n","    if token.startswith(special_char):\n","      detokenized_text+=\" \"+token[1:]\n","    else:\n","      detokenized_text+=token\n","    if ind==masked_index:\n","      detokenized_text+=\"**\"\n","  return detokenized_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"psfmg4EvMRPA","outputId":"9f6cee9a-ed36-4a5b-baa0-6d67d35f24c1","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["#predicted_index = \n","\n","predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)[0]\n","pred_text = tokenized_text.copy()\n","pred_text[masked_index] = predicted_token\n","print(\"PREDICTION TOKENIZED TEXT : {}\".format(pred_text))\n","print(\"PREDICTION DETOKENIZED TEXT : {}\".format(detokenized_text(pred_text, masked_index,special_char=\"▁\")))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PREDICTION TOKENIZED TEXT : ['<s>', '▁Les', '▁feux', '▁de', '▁b', 'rousse', '▁qui', '▁s', 'év', 'issent', '▁depuis', '▁septembre', '▁en', '▁Guinée', ',', '▁favorisé', 's', '▁par', '▁des', '▁températures', '▁exceptionnelles', ',', '▁dépassent', '▁tous', '▁les', '▁records', '.', '</s>']\n","PREDICTION DETOKENIZED TEXT : <s> Les feux de brousse qui sévissent depuis septembre en ** Guinée**, favorisés par des températures exceptionnelles, dépassent tous les records.</s>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YhEzPZX0Tyil"},"source":["# Question 9. \n","# predicted_index  = list/tensor of top 5 prediction s"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vI0LCBN0xU_"},"source":["## Questions \n","\n","- Now do the same with your own text \n","- (if time) Do the same with another pretrained model (e.g. : bert-base-multilingual-cased the multilingual version of BEERT)"]},{"cell_type":"markdown","metadata":{"id":"EBzm0CBL65za"},"source":["# Fine Tuning for Sequence Classification : Sentiment Analysis\n","\n","We will now apply fine-tuning on the original version of BERT (bert-uncased)\n","\n","\n","## Download data \n","\n","1- First download the data with https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8 \n","\n","2- Unzip it in your labtob\n","\n","\n","3-  upload each tsv file (train.tsv, dev.tsv and test.tsv): on the left panel click on the bottom most folder symbol, then \"Import\" button\n","\n","## Preprocessing \n","\n","Then, we introduce a few preprocessing function to help you get to the model. \n","\n","**SSTDataset is a class that handle get, tokenization and padding of the sentences.**\n"]},{"cell_type":"code","metadata":{"id":"9eZCx9LnoNkx","outputId":"15a89466-cb3d-44de-fe2f-c9ae62959607","colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from transformers import BertTokenizer, AutoModel, AutoTokenizer\n","import pandas as pd\n","from torch.utils.data import DataLoader\n","\n","class SSTDataset(Dataset):\n","\n","    def __init__(self, filename, maxlen, model_name='bert-base-uncased'):\n","\n","        #Store the contents of the file in a pandas dataframe\n","        self.df = pd.read_csv(filename, delimiter = '\\t')\n","\n","        #Initialize the BERT tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        self.maxlen = maxlen\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","\n","        #Selecting the sentence and label at the specified index in the data frame\n","        sentence = self.df.loc[index, 'sentence']\n","        label = self.df.loc[index, 'label']\n","\n","        #Preprocessing the text to be suitable for BERT\n","        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n","        if self.tokenizer.cls_token is None:\n","          bos_token = self.tokenizer.bos_token\n","        else:\n","          bos_token = self.tokenizer.cls_token\n","          \n","        if self.tokenizer.sep_token is None:\n","          eos_token = self.tokenizer.eos_token\n","        else:\n","          eos_token = self.tokenizer.sep_token\n","        \n","        tokens = [bos_token] + tokens + [eos_token] #Insering the CLS and SEP token in the beginning and end of the sentence\n","        if len(tokens) < self.maxlen:\n","            tokens = tokens + [self.tokenizer.pad_token for _ in range(self.maxlen - len(tokens))] #Padding sentences\n","        else:\n","            tokens = tokens[:self.maxlen-1] + [eos_token] #Prunning the list to be of specified max length\n","\n","        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n","        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n","        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n","        attn_mask = (tokens_ids_tensor != 0).long()\n","\n","        return tokens_ids_tensor, attn_mask, label"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"shAK6RMroWW0"},"source":["\n","#Creating instances of training and validation set\n","train_set = SSTDataset(filename = 'train.tsv', maxlen = 30, model_name='bert-base-uncased')\n","val_set = SSTDataset(filename = 'dev.tsv', maxlen = 30, model_name='bert-base-uncased')\n","\n","#Creating intsances of training and validation dataloaders\n","train_loader = DataLoader(train_set, batch_size = 12, num_workers = 5)\n","val_loader = DataLoader(val_set, batch_size = 12, num_workers = 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"41hGgQShpn2y"},"source":["## Data \n","\n","We define the SSTDataset class. It is a compact wrapper to :\n","-  access data for the sentiment analysis dataset more easily\n","- tokenize into subwords for our languag model\n"]},{"cell_type":"code","metadata":{"id":"KzbIWZhFpcjF","outputId":"e108ec49-45be-406b-e7c9-d28de9f08525","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# get the number of sentences\n","print(val_set.__len__())\n","# get  tokenized sentence indexed by 1 \n","val_set.__getitem__(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["872\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(tensor([  101,  4895, 10258,  2378,  8450,  2135, 21657,  1998,  7143,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0]),\n"," 0)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"7fXL46LZqhyp"},"source":["## Define the Sentiment Analysis model using pytorch \n","\n","- As we have seen in the last lab session all pytorch model follow the same template \n","  - One class withto instansiate the model \n","  - forward() method to define the foward pass\n","\n","- Here, we will use the pretrained Masked Language Model as one module of our sentiment analysis model "]},{"cell_type":"code","metadata":{"id":"LCzcv_tWrEA5"},"source":["class SentimentClassifier(nn.Module):\n","\n","    def __init__(self, pretrained_model_name='bert-base-uncased'):\n","        super(SentimentClassifier, self).__init__()\n","        \n","        #Loading Mask Language Model \n","        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n","        #we append an extra layer for Classification (it will be randomly initialized)\n","        self.cls_layer = nn.Linear(self.encoder.pooler.dense.out_features, 1)\n","\n","    def forward(self, seq, attn_masks):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","        '''\n","\n","        #Feeding the input to BERT model to obtain contextualized representations\n","        # see in the hugging face doc what to input\n","        #cont_reps = #  self.encoder(..)..\n","\n","        #Obtaining the representation of [CLS] head\n","        cls_rep = cont_reps[:, 0]\n","\n","        #Feeding cls_rep to the classifier layer\n","        logits = self.cls_layer(cls_rep)\n","\n","        return logits\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eb0-o8qHrosL"},"source":["# we now instansiate the model \n","sentiment_model = SentimentClassifier('bert-base-uncased')\n","# if gpu mode\n","sentiment_model = sentiment_model.to(\"cuda\")\n","# to check if the weights of the model are in gpu : \n","# sentiment_model.cls_layer.weight.is_cuda\n","# can checkout all the layers by running \n","#sentiment_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cO8IBEsqtQFS"},"source":["## Define Training Process\n","\n","- a loss \n","\n","We are doing binary cl\n","\n","- an optimizer \n","\n","Here we will use a variant of the Stochastic Gradient Descent called ADAM (cf. reference at the top) \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"hmAvFpk7tNyU"},"source":["import torch.nn as nn\n","import torch.optim as optim\n","# define the loss and optimizer \n","criterion = nn.BCEWithLogitsLoss()\n","opti = optim.Adam(sentiment_model.parameters(), lr = 2e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k08QY9JMvF4i"},"source":["# Training loop\n","\n"]},{"cell_type":"code","metadata":{"id":"vSCBgEfivHGY"},"source":["import pdb\n","def train(model, criterion, opti, train_loader, val_loader, max_eps=1, gpu=False, print_every=1,validate_every=1, break_training_after=None):\n","    if gpu:\n","      model = model.to(\"cuda\")\n","    for ep in range(max_eps):\n","        \n","        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n","            #Clear gradients\n","            opti.zero_grad()  \n","            #Converting these to cuda tensors\n","            if gpu:\n","              seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n","            #Obtaining the logits from the model\n","            logits = model(seq, attn_masks)\n","\n","            #Computing loss\n","            loss = criterion(logits.squeeze(-1), labels.float())\n","\n","            #Backpropagating the gradients\n","            loss.backward()\n","\n","            #Optimization step\n","            opti.step()\n","            if (it + 1) % print_every == 0:\n","                accuracy = torch.sum((logits>0).int().squeeze(1)==labels)/float(labels.size(0))\n","                print(\"Iteration {} of epoch {} complete. Loss : {}, Accuracy {} \".format(it+1, ep+1, loss.item(),accuracy))\n","            if break_training_after is not None and it>break_training_after:\n","              print(\"Early breaking : did not cover a full epoch but only {} iteration \".format(it))\n","              break\n","        if ep % validate_every==0:\n","          # evaluation on the validation set \n","          n_batch_validation = 0\n","          loss_validation = 0\n","          accuracy_validation = 0\n","          for it, (seq, attn_masks, labels) in enumerate(val_loader):            \n","            if gpu:\n","              seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n","            #Obtaining the logits from the model\n","            logits_val = model(seq, attn_masks)\n","            n_batch_validation+=1\n","            #Computing loss\n","           \n","            _loss = float(criterion(logits_val.squeeze(-1), labels.float()))\n","            _accu = float(torch.sum((logits_val>0).int().squeeze(1)==labels)/float(labels.size(0)))\n","           \n","            loss_validation += _loss\n","            accuracy_validation += _accu\n","          print(\"EVALUATION Validation set : mean loss {} n mean accuracy {}\".format(loss_validation/n_batch_validation, accuracy_validation/n_batch_validation))\n","\n","          "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"juL5Q6jqHwAg","outputId":"3aa6804d-41ec-44f7-c8fd-28bfd106806e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train(sentiment_model, criterion, opti, train_loader, val_loader,max_eps=5, print_every=100, gpu=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration 100 of epoch 1 complete. Loss : 0.6502393484115601, Accuracy 0.6666666865348816 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AVxxuXNyuqpO"},"source":["## Questions \n","\n","10- Plot loss and accuracy   \n","11- Compare different value of the learning rate in the adam Optimizer (between 1e-6 and 5e-5)  \n","12- Conclude on the performance of BERT on sentiment analysis \n","13- Now do the same choosing another pretrained model \n","\n","- ex : bert-large-uncased (much larger version of BERT) bert-base-multilingual-cased (multilingual version of BERT)\n","\n","14- Conclude on what is the best pretraining model for sentiment analysis\n"]},{"cell_type":"code","metadata":{"id":"g0dCT6hrvnRa"},"source":[""],"execution_count":null,"outputs":[]}]}