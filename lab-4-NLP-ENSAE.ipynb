{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"lab-session-4-ML-for-NLP-ENSAE.ipynb","provenance":[],"collapsed_sections":["iVTZd85T3fIY","yyf3KO_a3fKA"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"G9Guq-DP3fD2","colab_type":"text"},"source":["### ----  \n","\n","requirements (if run locally) : \n","- `conda create -n td4 python=3.6`\n","- `source activate td4`\n","- `pip install jupyter`\n","- `pip install torch torchvision`\n","- `conda install -c conda-forge spacy `\n","- `python -m spacy download en_core_web_sm`\n","- `cd ./td4`\n","- `jupyter notebook`\n","\n","### ----  \n","\n","\n","\n","# Machine Learning for NLP : TD 4 \n","## _Description_\n","\n","### Course takeaways\n","\n","### TD outline \n","\n","1. Introduction to pytorch\n","2. Sequence Labelling with pytorch\n","\n","\n","### Resources : \n","\n","https://pytorch.org/tutorials/  \n","https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html    \n","https://stats.stackexchange.com/questions/268202/backpropagation-algorithm-nn-with-rectified-linear-unit-relu-activation   \n","https://ruder.io/optimizing-gradient-descent/"]},{"cell_type":"markdown","metadata":{"id":"N30J_iCf3fEC","colab_type":"text"},"source":["## Pytorch\n","PyTorch is a Python based library for scientific computing that provides three main features:\n","- An n-dimensional Tensor, which is similar to numpy but can run on GPUs\n","- Easily build big computational graphs for deep learning\n","- Automatic differentiation for computing gradients \n","\n","Usages : \n","- It’s a Python-based scientific computing package targeted at two sets of audiences:\n","    - A replacement for NumPy to use the power of GPUs\n","    - a deep learning research platform that provides maximum flexibility and speed\n"]},{"cell_type":"markdown","metadata":{"id":"3II9uLKW3fEF","colab_type":"text"},"source":["## Pytorch basics\n","\n","**NB** : Tensor are the basics block of pytorch. Tensor allows to store data (input data or target data) as well as the parameters (also called weights, neurons,...) of your neural network.\n","\n","\n","- tensor creation \n","- tensor types \n","- basic operations between tensors\n","- from and to numpy \n","- about GPU "]},{"cell_type":"code","metadata":{"id":"FITe7Hhp3fEK","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDkgzVe13fEd","colab_type":"text"},"source":["### Tensors\n","\n","\n","**What is a pytorch tensor ?** : A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n","\n","Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n","\n","**How to define a pytoch tensor ?**\n","- using existing constructors : _torch.ones_ , _torch.zeros_ _torch.rand_\n","- based on existing object\n","    - from another tensor (or only using the shape of the other tensor)\n","    - from a python list \n","    - from a numpy array"]},{"cell_type":"code","metadata":{"id":"DzPgaeY13fEn","colab_type":"code","outputId":"32d21fb0-7e0d-4cc6-af0b-4ae1272ae046","executionInfo":{"status":"ok","timestamp":1583153224465,"user_tz":-60,"elapsed":1368,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["# define \n","ones = torch.ones(3,2)\n","# a tensor can be printed\n","print(ones)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.],\n","        [1., 1.]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-eW3NOwG3fFE","colab_type":"code","outputId":"844c3b85-52e9-4138-cd83-8e61e6785707","executionInfo":{"status":"ok","timestamp":1583153226042,"user_tz":-60,"elapsed":1631,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["# other basic definition \n","print(torch.zeros(5,3), \"\\n\", \n","      torch.rand(2,3), \"\\n\", \n","      torch.empty(2,2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]]) \n"," tensor([[0.4385, 0.6439, 0.6972],\n","        [0.5278, 0.9691, 0.7974]]) \n"," tensor([[2.3786e-35, 0.0000e+00],\n","        [0.0000e+00, 0.0000e+00]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nS_i19lt3fFQ","colab_type":"code","outputId":"fe2944f9-1d74-4c48-c446-226cf1d52cda","executionInfo":{"status":"ok","timestamp":1583153226693,"user_tz":-60,"elapsed":923,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["# from a python list \n","ls = [[[1,3,5,6],[-1,4,4,4]],[[-1,-3,-5,-6],[10,-4,-4,-4]]]\n","tensor = torch.Tensor(ls)\n","print(tensor)\n","# from a numpy array : \n","array = np.array([0,1])\n","#array\n","tensor = torch.from_numpy(array)\n","print(tensor)\n","# symetrically  tensor.numpy()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[[ 1.,  3.,  5.,  6.],\n","         [-1.,  4.,  4.,  4.]],\n","\n","        [[-1., -3., -5., -6.],\n","         [10., -4., -4., -4.]]])\n","tensor([0, 1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tRaSA_R-3fFp","colab_type":"code","outputId":"17670656-71a0-49d5-f959-b70a8cc133b8","executionInfo":{"status":"error","timestamp":1583153241951,"user_tz":-60,"elapsed":1045,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":178}},"source":["# list must be in a proper matrix shape\n","ls = [[[1,3,5,6],[-1,4,4,4]],[[-1,-3,-5,-6],[10,-4,-4]]]\n","torch.Tensor(ls)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0cbd3fccd9e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 2 (got 3)"]}]},{"cell_type":"markdown","metadata":{"id":"X-KGQ2hK3fGe","colab_type":"text"},"source":["**Basic manipulations**\n","- access type / change data types \n","- access elements \n","- reshape \n","- maths opertions : add, multiply , ..\n","- differentiate / derive\n","- set to a specific _device_ : GPU , GPU:0, GPU:1 , CPU ..."]},{"cell_type":"code","metadata":{"id":"Eu2aGF3j3fGt","colab_type":"code","outputId":"432869dd-2c3b-4449-9c64-725eb5b5fdf0","executionInfo":{"status":"ok","timestamp":1583153249624,"user_tz":-60,"elapsed":513,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# get type \n","print(tensor,tensor.dtype)\n","# change type \n","tensor = tensor.float()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([0, 1]) torch.int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yz2gxNen3fHS","colab_type":"text"},"source":["**NB** : types are important in Deep Learning  because : \n","- some types are more memory consumming than others : e.g : float16 vs float32\n","- some operations require specific type (cf. Embedding layer ...)"]},{"cell_type":"code","metadata":{"id":"FThm_Rqv3fHg","colab_type":"code","outputId":"ac36b964-03f4-4c87-94e7-1d3a369b14e1","executionInfo":{"status":"ok","timestamp":1583153251734,"user_tz":-60,"elapsed":518,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":319}},"source":["tensor = torch.rand(5,2,2)\n","print(tensor)\n","# access one element\n","print(tensor[0,1,1])\n","# access several element\n","print(tensor[:3,0,:2])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[[0.7511, 0.0401],\n","         [0.3000, 0.1814]],\n","\n","        [[0.3512, 0.0374],\n","         [0.7119, 0.9846]],\n","\n","        [[0.9080, 0.3035],\n","         [0.0668, 0.7956]],\n","\n","        [[0.1612, 0.3080],\n","         [0.0748, 0.5270]],\n","\n","        [[0.4311, 0.0938],\n","         [0.5968, 0.4802]]])\n","tensor(0.1814)\n","tensor([[0.7511, 0.0401],\n","        [0.3512, 0.0374],\n","        [0.9080, 0.3035]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j5SGNZwf3fHv","colab_type":"text"},"source":["**NB** : pytorch tensor indexing exactly match numpy indexing"]},{"cell_type":"code","metadata":{"id":"GEuGn5Js3fHx","colab_type":"code","outputId":"32efa7b3-c7dd-40df-e256-757551d20e6b","executionInfo":{"status":"ok","timestamp":1583153254265,"user_tz":-60,"elapsed":515,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"source":["# get the shape of a tensor\n","tensor.size()\n","# reshape it \n","print(tensor, \"\\n\",\n","      tensor.view(2,2,5))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[[0.7511, 0.0401],\n","         [0.3000, 0.1814]],\n","\n","        [[0.3512, 0.0374],\n","         [0.7119, 0.9846]],\n","\n","        [[0.9080, 0.3035],\n","         [0.0668, 0.7956]],\n","\n","        [[0.1612, 0.3080],\n","         [0.0748, 0.5270]],\n","\n","        [[0.4311, 0.0938],\n","         [0.5968, 0.4802]]]) \n"," tensor([[[0.7511, 0.0401, 0.3000, 0.1814, 0.3512],\n","         [0.0374, 0.7119, 0.9846, 0.9080, 0.3035]],\n","\n","        [[0.0668, 0.7956, 0.1612, 0.3080, 0.0748],\n","         [0.5270, 0.4311, 0.0938, 0.5968, 0.4802]]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VD2AQmQH3fIN","colab_type":"code","outputId":"71d8ef05-5292-4699-dbde-d9bcadea1e91","executionInfo":{"status":"ok","timestamp":1583153284891,"user_tz":-60,"elapsed":474,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["intTensor = torch.ones(3,2, dtype=torch.float32)\n","print(intTensor, intTensor.dtype)\n","intTensor.int()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.],\n","        [1., 1.]]) torch.float32\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 1],\n","        [1, 1],\n","        [1, 1]], dtype=torch.int32)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"iVTZd85T3fIY","colab_type":"text"},"source":["### All operations on tensors \n","- all reshape \n","- squeeze \n","- sum , prod \n","- max, norm ..."]},{"cell_type":"markdown","metadata":{"id":"X0d2Wd-u3fIi","colab_type":"text"},"source":["## Automatic Differentiation \n","\n","The core component of any modern deep learning library is _Automatic Differentiation_. \n","\n","\n","**Recall**\n","- Training any deep learning model requires backpropagatation \n","- Backpropagation is an algorithm that efficiently computes the gradient of a neural network's output based on its input and with regard to all its parameters (or also named weights)\n","\n","_Automatic Differentiation_ provides a way of automatically computing gradients of any function. In other words, _automatic differentiation_ gives you the possibility to build complex neural network without caring about computing the gradients by yourself. \n","\n","\n","**NB** \n","\n","Having access to an open source library that performs Automatic Differentation (tensorflow/pytorch and before Dynet or Theano..) is one of the reasons for the popularity and sucess of Deep Learning today.\n","\n","### Automatic Differentiation in a nutshell\n","\n","\n","**Definition**\n","Automatic differentiation refers to a general way of taking a program which computes a value, and automatically constructing a procedure for computing derivatives of that value.\n","\n","Automatic Differentation requires 3 steps \n","\n","1. Building a computation Graph \n","2. propagating inputs throughout the graph (forward pass)\n","3. Computing gradient of each of the node in the graph (backward pass)"]},{"cell_type":"code","metadata":{"id":"8ufdSVqe3fIn","colab_type":"code","outputId":"0c7453d0-1b6a-4d76-968b-c278bfc01d31","executionInfo":{"status":"ok","timestamp":1583153293444,"user_tz":-60,"elapsed":607,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x = torch.ones(2, 2, requires_grad=True)\n","# double checking if gradient \n","print(\"Checking gradient is set to {}. Its gradient is still {} \".format(x.requires_grad, x.grad))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Checking gradient is set to True. Its gradient is still None \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gm23ooct3fIy","colab_type":"code","outputId":"74564580-2441-45ac-c766-c1c8a9c9370a","executionInfo":{"status":"ok","timestamp":1583153295357,"user_tz":-60,"elapsed":528,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# let us define a basic operation\n","y = x+1\n","print(y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[2., 2.],\n","        [2., 2.]], grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V6y3ScPX3fI6","colab_type":"code","outputId":"a5d94461-e151-42ce-cdb5-814b14165c06","executionInfo":{"status":"ok","timestamp":1583153297833,"user_tz":-60,"elapsed":458,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# y has now a gradient attribute , grad is none\n","y.grad_fn, y.grad"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<AddBackward0 at 0x7f37e11a7ba8>, None)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"U-DRTAqB3fJF","colab_type":"code","outputId":"62419ee6-9c0c-42c2-eedb-4fb2d562df8a","executionInfo":{"status":"ok","timestamp":1583153298888,"user_tz":-60,"elapsed":447,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["z = y * y * 3\n","out = z.mean()\n","print(z, out, z.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[12., 12.],\n","        [12., 12.]], grad_fn=<MulBackward0>) tensor(12., grad_fn=<MeanBackward0>) None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TPBP87F13fJK","colab_type":"code","outputId":"4dde79ba-896a-44ba-9dfc-3c1e126e4641","executionInfo":{"status":"ok","timestamp":1583153300009,"user_tz":-60,"elapsed":472,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["out.backward()\n","# Let's inspect the gradient at each previous variable' gradients now\n","print(\"Gradients with regard to intermediate nodes:\", out.grad, z.grad, y.grad)\n","print(\"Gradients with regard to the input node that we considered to be the parameter:\", x.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Gradients with regard to intermediate nodes: None None None\n","Gradients with regard to the input node that we considered to be the parameter: tensor([[3., 3.],\n","        [3., 3.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"etJYZGzda-i-","colab_type":"text"},"source":["### Questions:\n","- Find the function that is being differentiated with regard to x_ij.\n","- Try to manually retrieve the same gradient with the function you found for x=[[1, 1], [1, 1]]."]},{"cell_type":"code","metadata":{"id":"xVPoBeXf3fJR","colab_type":"code","outputId":"eabed9d1-8d9b-40e1-f1d9-88b09f05807b","executionInfo":{"status":"ok","timestamp":1583153304966,"user_tz":-60,"elapsed":549,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# to manipulate a tensor without its gradient \n","out.detach()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(12.)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"cROAukmM3fJW","colab_type":"text"},"source":["## Pytorch Model\n","\n","\n","Our goal is to define a deep learning model, train it, make prediction with it and evaluate it. \n","\n","With pytorch this means doing the three following \"scripts\" : \n","1. Defining the model \n","2. Implementing the prediction \n","3. Implementing the training loop \n","    - Defining a loss\n","    - Defining an optimizer\n","    - Loop :\n","        - forward pass \n","        - backward pass\n","        - applying optimization update rule\n","4. Evaluating the model / playing with it \n","    - You can use the training criteria (loss) as your evaluation score\n","    - You can use another score : accuracy, F1 , ..."]},{"cell_type":"markdown","metadata":{"id":"qgIXi-gu3fJb","colab_type":"text"},"source":["### 1. Defining the model \n","Pytorch models always follow the same template : \n","\n","- a class\n","- defining all layers (or parameters) in _init_()\n","- defining the forward pass in foward()\n","\n","Let's see what it looks like with a simple 2 layers model.\n","\n","All trivial Neural Network layers can generally be found in [torch.nn](https://pytorch.org/docs/stable/nn.html).\n","\n","**Warning**: All your parametrized modules (Layers or any trainable vectors) must be defined as *direct* attributes to your ```nn.Module``` class so that the call to ```.backward()``` can properly propagate the gradients through everything. To define layers in list attribute, (resp. dictionary attributes) use ```ModuleList``` (resp. ```ModuleDict```).\n","\n","\n","<img src=\"./imgs/nn.png\">\n","\n"]},{"cell_type":"code","metadata":{"id":"c9CXT3hd3fJd","colab_type":"code","colab":{}},"source":["# defining the model \n","class MyModel(torch.nn.Module):\n","    def __init__(self, D_in, H, D_out):\n","        \"\"\"\n","        In the constructor we instantiate two nn.Linear modules and assign them as\n","        member variables.\n","        \"\"\"\n","        super(MyModel, self).__init__()\n","        self.linear1 = torch.nn.Linear(D_in, H, bias=True)\n","        self.linear2 = torch.nn.Linear(H, D_out, bias=True)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        In the forward function we accept a Tensor of input data and we must return\n","        a Tensor of output data. We can use Modules defined in the constructor as\n","        well as arbitrary operators on Tensors.\n","        \"\"\"\n","        h_relu = torch.relu(self.linear1(x))\n","        y_pred = self.linear2(h_relu)\n","        return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKt4AmzG3fJp","colab_type":"text"},"source":["### 2. Forward pass \n","1. instanciating the model\n","2. getting input data \n","3. computing the foward pass"]},{"cell_type":"code","metadata":{"id":"hkclHYpb3fJr","colab_type":"code","outputId":"a2442201-167f-4ea8-f2d6-2e216458c316","executionInfo":{"status":"ok","timestamp":1583153328174,"user_tz":-60,"elapsed":480,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["# instanciating the model \n","N, D_in, H, D_out = 2, 10, 10, 2\n","\n","# Construct our model by instantiating the class defined above \n","# Note: all the parameters are initialized here \n","model = MyModel(D_in, H, D_out)\n","# You can look up into the model \n","model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MyModel(\n","  (linear1): Linear(in_features=10, out_features=10, bias=True)\n","  (linear2): Linear(in_features=10, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"fHDPw4BL3fJw","colab_type":"code","colab":{}},"source":["# Create random Tensors to hold inputs and outputs\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_IRdn853fJ9","colab_type":"code","outputId":"e0cc4937-350a-45db-ceef-7a6ea2a00a8b","executionInfo":{"status":"ok","timestamp":1583153335035,"user_tz":-60,"elapsed":489,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["#model = MyModel(D_in, H, D_out)\n","# forward pass / predict x \n","y_pred = model(x) # almost equivalent to model.forward(x)\n","# y_pred\n","y_pred"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1292, -0.0250],\n","        [-0.1046, -0.0991]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"yyf3KO_a3fKA","colab_type":"text"},"source":["### Questions \n","- Why do the prediction change if the model is re-instanciated ? \n","- Can this be a problem ? \n","- How to avoid it ? "]},{"cell_type":"markdown","metadata":{"id":"yAQ3qFR63fKB","colab_type":"text"},"source":["### 3. Training loop \n","\n","- Criterion : \n","\n","a model is trained with regard to a _training criterion_ or a _loss_.   \n","Pytorch provides many different pre-coded losses : \n","    - Mean-Square Error \n","    - Categorical Cross-Entropy , ...\n","\n","Most of them can be found in [torch.nn](https://pytorch.org/docs/stable/nn.html) \n","- Optimizer \n","\n","In pytorch as in any deep learning framwork, models are trained with backpropagation. Backpropagation consists in applying Stochastic Gradient Descent (SGD) to a neural network. There is a broad range of variants around the simple form of SGD. \n","\n","Pytorch provides pre-defined objects for many different forms of Gradient Descent algorithm in [torch.optim](https://pytorch.org/docs/stable/optim.html):\n","- SGD \n","- Adadelta \n","- Adam \n","\n","Your optimizer will be instanciated with it's configuration(*e.g.* the *step_size* or *learning_rate* for SGD), and the network's parameters.\n","\n","Overview of all the Gradient Descent based algorithms : https://ruder.io/optimizing-gradient-descent/ \n","\n","\n","- Training Loop :\n","    - forward pass to get prediction and the loss value \n","    - zero_grad : Resetting the gradient value to zero for all parameters before adding their newly backpropagated values) \n","    - compute the gradients' value with loss.backward()\n","    - update all the parameters of the model with optimizer.step()\n","\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"l9n81Knq3fKD","colab_type":"code","outputId":"9635b62c-24b3-4bed-d0b5-d9d18d4fa850","executionInfo":{"status":"ok","timestamp":1583153351441,"user_tz":-60,"elapsed":4465,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# instanciate the model \n","# Note: all the model parameters are intialized at this step\n","model = MyModel(D_in, H, D_out)\n","\n","criterion = torch.nn.MSELoss(reduction='mean')\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n","\n","# t is normally also an index over the samples (or batches) in your dataset,\n","# but we will just consider it to be a time-step here\n","for t in range(10000):\n","    # Forward pass: Compute predicted y by passing x to the model\n","    y_pred = model(x)\n","\n","    # Compute and print loss\n","    loss = criterion(y_pred, y)\n","    if t % 100 == 0:\n","        print(\"Step:{} Loss:{} \".format(t, loss.item()))\n","\n","    # Zero gradients, perform a backward pass, and update the weights.\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Step:0 Loss:0.21324831247329712 \n","Step:100 Loss:0.20180049538612366 \n","Step:200 Loss:0.19123151898384094 \n","Step:300 Loss:0.18145005404949188 \n","Step:400 Loss:0.1723765879869461 \n","Step:500 Loss:0.1639418601989746 \n","Step:600 Loss:0.15608489513397217 \n","Step:700 Loss:0.14875219762325287 \n","Step:800 Loss:0.14189618825912476 \n","Step:900 Loss:0.13547523319721222 \n","Step:1000 Loss:0.12945178151130676 \n","Step:1100 Loss:0.12379277497529984 \n","Step:1200 Loss:0.11846837401390076 \n","Step:1300 Loss:0.1134520173072815 \n","Step:1400 Loss:0.10871973633766174 \n","Step:1500 Loss:0.10425002872943878 \n","Step:1600 Loss:0.10002334415912628 \n","Step:1700 Loss:0.0960220992565155 \n","Step:1800 Loss:0.09223027527332306 \n","Step:1900 Loss:0.08863332122564316 \n","Step:2000 Loss:0.08521801233291626 \n","Step:2100 Loss:0.08197227865457535 \n","Step:2200 Loss:0.078885018825531 \n","Step:2300 Loss:0.07594609260559082 \n","Step:2400 Loss:0.07314622402191162 \n","Step:2500 Loss:0.07047688215970993 \n","Step:2600 Loss:0.06793016195297241 \n","Step:2700 Loss:0.06549879163503647 \n","Step:2800 Loss:0.06317602097988129 \n","Step:2900 Loss:0.06095562130212784 \n","Step:3000 Loss:0.05883193016052246 \n","Step:3100 Loss:0.056799501180648804 \n","Step:3200 Loss:0.05485339090228081 \n","Step:3300 Loss:0.05298898369073868 \n","Step:3400 Loss:0.05120188742876053 \n","Step:3500 Loss:0.04948816075921059 \n","Step:3600 Loss:0.047844015061855316 \n","Step:3700 Loss:0.046265922486782074 \n","Step:3800 Loss:0.04475058242678642 \n","Step:3900 Loss:0.04329492524266243 \n","Step:4000 Loss:0.0418960377573967 \n","Step:4100 Loss:0.040551237761974335 \n","Step:4200 Loss:0.03925792872905731 \n","Step:4300 Loss:0.03801368921995163 \n","Step:4400 Loss:0.036816250532865524 \n","Step:4500 Loss:0.03566349670290947 \n","Step:4600 Loss:0.03455337882041931 \n","Step:4700 Loss:0.03348400071263313 \n","Step:4800 Loss:0.032453570514917374 \n","Step:4900 Loss:0.03146035224199295 \n","Step:5000 Loss:0.03050278127193451 \n","Step:5100 Loss:0.02957927994430065 \n","Step:5200 Loss:0.028688427060842514 \n","Step:5300 Loss:0.027828814461827278 \n","Step:5400 Loss:0.02699916623532772 \n","Step:5500 Loss:0.02619822323322296 \n","Step:5600 Loss:0.025424834340810776 \n","Step:5700 Loss:0.024677824229002 \n","Step:5800 Loss:0.023956164717674255 \n","Step:5900 Loss:0.023258855566382408 \n","Step:6000 Loss:0.022584909573197365 \n","Step:6100 Loss:0.0219334214925766 \n","Step:6200 Loss:0.021303478628396988 \n","Step:6300 Loss:0.02069425769150257 \n","Step:6400 Loss:0.020104993134737015 \n","Step:6500 Loss:0.0195348858833313 \n","Step:6600 Loss:0.01898321695625782 \n","Step:6700 Loss:0.01844928227365017 \n","Step:6800 Loss:0.017932429909706116 \n","Step:6900 Loss:0.01743203215301037 \n","Step:7000 Loss:0.01694745197892189 \n","Step:7100 Loss:0.01647813618183136 \n","Step:7200 Loss:0.016023488715291023 \n","Step:7300 Loss:0.015583009459078312 \n","Step:7400 Loss:0.015156197361648083 \n","Step:7500 Loss:0.014742542989552021 \n","Step:7600 Loss:0.01434155274182558 \n","Step:7700 Loss:0.013952826149761677 \n","Step:7800 Loss:0.01357589103281498 \n","Step:7900 Loss:0.013210362754762173 \n","Step:8000 Loss:0.012855847366154194 \n","Step:8100 Loss:0.012511943466961384 \n","Step:8200 Loss:0.012178330682218075 \n","Step:8300 Loss:0.011854605749249458 \n","Step:8400 Loss:0.01154044084250927 \n","Step:8500 Loss:0.011235532350838184 \n","Step:8600 Loss:0.010939572006464005 \n","Step:8700 Loss:0.0106522087007761 \n","Step:8800 Loss:0.010373224504292011 \n","Step:8900 Loss:0.010102279484272003 \n","Step:9000 Loss:0.009839161299169064 \n","Step:9100 Loss:0.009583587758243084 \n","Step:9200 Loss:0.009335337206721306 \n","Step:9300 Loss:0.00909414328634739 \n","Step:9400 Loss:0.008859798312187195 \n","Step:9500 Loss:0.008632061071693897 \n","Step:9600 Loss:0.008410719223320484 \n","Step:9700 Loss:0.008195600472390652 \n","Step:9800 Loss:0.007986491546034813 \n","Step:9900 Loss:0.0077832164242863655 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kWiyDkgd3fKL","colab_type":"text"},"source":["If the Loss decrease it means the gradient descent is working !! \n","\n","**Note:** Don't forget the zero_grad()\n","- we are doing gradient backpropagation at each step \n","- gradients are computed with the loss.backward \n","- after each update we must set to zero all the gradients values otherwise they get accumulated (hence zero_grad())"]},{"cell_type":"markdown","metadata":{"id":"HHQDkr5d3fKN","colab_type":"text"},"source":["### Questions : \n","\n","- plot the loss values that you would record while going through the above loop"]},{"cell_type":"code","metadata":{"id":"32iy09sZlMqo","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","from torch.optim import SGD, Adam\n","\n","def plot_loss(lr, N, optim_alg, target=None, max_steps=None):\n","    \"\"\"\n","    This function takes as arguments:\n","    lr: the learning rate\n","    N: the number of samples\n","    optim_alg: the optimisation algorithm\n","    target: a target value for the loss ( a message should printed when this \n","            target is reached)\n","    max_steps: the number of gradient steps for the optimisation process\n","    \"\"\"\n","    pass\n","\n","\n","plt.title('lr: {}, N:{}, optim_alg:{}'.format(1e-4, 2, 'SGD'))\n","plot_loss(lr=1e-4, N=2, optim_alg=SGD)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7GlPukglC8E","colab_type":"text"},"source":["\n","- how many steps do you need for the loss to reach 1e-1 ? "]},{"cell_type":"code","metadata":{"id":"gVJ7w4EPlNV2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6Q_z8HElOlt","colab_type":"text"},"source":["\n","- now same question if\n","        - lr=1e-5,1-3 \n","        - N = 10, 100, 1000 (NB : you can )\n","        - optim.Adam "]},{"cell_type":"code","metadata":{"id":"X9RS6C-glPXp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N851da1slQGQ","colab_type":"text"},"source":["\n","- What can you conclude on the capacity of a neural network ? \n"]},{"cell_type":"markdown","metadata":{"id":"V3mHFJZR3fKO","colab_type":"text"},"source":["## Sequence Labelling with pytorch\n","\n","Now that we have seen how to build a simple neural network, let's build a model for a task more useful in NLP : _Sequence Labelling_ \n","\n","Recall : Sequence labelling is the task of predicting a label to a sequence among a fixed range of possibilities \n","\n","e.g : Sentiment Analysis \n","\n","\n","<img src=\"./imgs/sentiment_analysis.png\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ms6k_Gdv3fKQ","colab_type":"text"},"source":["### 1. Define the model\n","Define a neural network that uses an LSTM ([nn.LSTM](https://pytorch.org/docs/stable/nn.html)) to classify the elements of a sequence.\n","\n"," The input sequence will be given to the neural network as a series of indexes. Each index (corresponding to a token in the source vocabulary) will be transformed to a corresponding trainable embedding ([nn.Embedding](https://pytorch.org/docs/stable/nn.html)) before entering the LSTM.   "]},{"cell_type":"code","metadata":{"id":"woz45Oab3fKR","colab_type":"code","colab":{}},"source":["class SequenceLabeller(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes, sequence_model=\"LSTM\"):\n","        super(SequenceLabeller, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        # self.word_embeddings = ??\n","        \n","        if sequence_model == \"LSTM\":\n","            pass\n","            # The LSTM takes word embeddings as inputs, and outputs hidden states\n","            # with dimensionality hidden_dim.\n","            # self.seq = ??\n","        else:\n","            raise(Exception(\"Sequence model {} not supported\".format(sequence_model)))\n","\n","        # The linear layer that maps from hidden state space to class space\n","        # self.hidden2tag = ??\n","\n","    def forward(self, sentence):\n","          pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4VNxs5Vk3fKW","colab_type":"text"},"source":["### 2.1 Prepare the data\n","First, put the three files given [here](https://drive.google.com/drive/folders/19fFgwB0Vk9mfGcA2TNhIeViBHYR4zATX?usp=sharing) in your working folder. Then, choose a data file to perform your sequence labelling and use the given loop to read it.\n","*optional* \n","- inspect the data files\n","- try to guess how the data is being parsed by the loop\n","- inspect the ``re`` package [documentation](https://docs.python.org/3/library/re.html) and see whether you were right."]},{"cell_type":"code","metadata":{"id":"jYSg1-z03fK4","colab_type":"code","colab":{}},"source":["import re\n","import spacy\n","tokenizer = spacy.load(\"en_core_web_sm\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e82CihiY3fK9","colab_type":"code","colab":{}},"source":["\n","def get_data(path):\n","  data = []\n","  no_match = 0\n","  with open(path, \"r\") as f:\n","      for line in f:\n","          match  = re.search(\"(.*)\\s\\s([0-1]+).*\", line)\n","          if match is not None:\n","              tokenized = tokenizer(match.group(1).strip())\n","              sent = [token.text for token in  tokenized]\n","              score = match.group(2)\n","              data.append((sent,int(score)))\n","          else:\n","            match  = re.search(\"(.*),([0-1]+).*\", line)\n","            if match is not None:\n","              tokenized = tokenizer(match.group(1).strip())\n","              sent = [token.text for token in  tokenized]\n","              score = match.group(2)\n","              data.append((sent,int(score)))\n","            else:\n","              no_match += 1\n","  return data, no_match\n","\n","data, no_match = get_data(\"./imdb_labelled.csv\") # fill in the path to a \n","                                                 # file of your choosing\n","training_data = data[:int(len(data)*4/5)]\n","test_data = data[int(len(data)*4/5):]\n","print(\"Got {} training examples, {} test examples, and failed to capture \"\n","\"{} examples.\".format(len(training_data), len(test_data), no_match))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OI3bPisL2Mt","colab_type":"text"},"source":["\n","Prepare:\n","- a structure that maps each token in your source vocabulary to a unique index.\n","- a structure that matches each index to it's corresponding token in the source vocabulary\n","- a structure that maps each label to an index\n","- a function that turns a token sequence to the corresponding index sequence\n","\n","The labels in this case are indexes themselves(`0`, `1`) but they can be otherwise (_e.g._ `positive`, `negative`, `amusing`, `anxious` ...)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"8lB643ke3fKX","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WAs2u9_QRBzU","colab_type":"text"},"source":["Inspect your data "]},{"cell_type":"code","metadata":{"id":"BrxSekQOI9JL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71smInd83fLD","colab_type":"text"},"source":["### 2.2 Forward pass\n","Instanciate the model and perform a forward pass on the first sentence in your data.  \n","See what the scores are before training.  \n","Note that element i,j of the output is the score for tag j for word i.  \n","Here we don't need to train, so the code is wrapped in `torch.no_grad()`"]},{"cell_type":"code","metadata":{"id":"nubhoMs03fLG","colab_type":"code","colab":{}},"source":["\n","with torch.no_grad():\n","  pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HmqMaU6E3fLM","colab_type":"text"},"source":["## Training \n","### 3.1  Optimizer and loss\n","Instanciate an optimizer and a loss function for your network from pytorch. "]},{"cell_type":"code","metadata":{"id":"2zti_PnN3fLN","colab_type":"code","colab":{}},"source":["# loss_function = ??\n","# optimizer = ??"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f5onpmKdYlqv","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"m7Ov4bpi3fLQ","colab_type":"text"},"source":["### 3.2 Training loop \n","Write a loop that goes through the data `n_epochs=40` times, and trains on it.  \n","The network should train all the word tags in a sentence to produce the entire sentence's label. This fuzzy kind of supervision is called weak-supervision (weakly-supervised learning). \n","tip: You should transform your target tags with [one_hot](https://pytorch.org/docs/stable/nn.functional.html#one-hot) before giving them to NLLLoss."]},{"cell_type":"code","metadata":{"id":"NDGhwRlU3fLU","colab_type":"code","colab":{}},"source":["from torch.nn.functional import one_hot\n","n_epochs = 40\n","for epoch in range(n_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n","    loss_mean_ep = 0\n","    n_sample = 0\n","    for sentence, tags in training_data:\n","        if len(sentence) < 2: continue\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        \n","        # Step 2. Get our inputs ready for the network, that is, turn them into\n","        # Tensors of word indices.\n","        #sentence_in = ??\n","        \n","        #targets = ??\n","        #one_hot_targets = ??\n","        \n","        # Step 3. Run our forward pass.\n","        #tag_scores = ??\n","        \n","        # Step 4. Compute the loss\n","        # loss = ??\n","        loss_mean_ep += loss\n","        n_sample += 1\n","\n","        # Backpropagate the loss and perform an optimisation step\n","        # ??\n","        # ??\n","\n","    print(\"Epoch {} loss {:0.4f} \".format(epoch, loss/n_sample))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0yns6Whi3fLa","colab_type":"text"},"source":["### 4. Evaluating model \n"]},{"cell_type":"markdown","metadata":{"id":"6AyKPKJFfVA5","colab_type":"text"},"source":["#### 1 - Evaluate your data qualitatively by inspecting 3 predictions for positive examples and 3 for negative ones on the test data"]},{"cell_type":"code","metadata":{"id":"T63LxBf_gAh3","colab_type":"code","colab":{}},"source":["with torch.no_grad():\n","  n_samples = 3\n","  for sentence, tag in test_data:\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyJANoAfgBJ_","colab_type":"text"},"source":["\n","#### 2 - Evaluate your data quantitatively by measuring the [roc_auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) on the test set, and generating a [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"]},{"cell_type":"code","metadata":{"id":"FhDpNLn_3fLc","colab_type":"code","colab":{}},"source":["from sklearn.metrics import roc_auc_score, classification_report\n","\n","ground_truth, scores = [], []\n","with torch.no_grad():\n","  for sentence, tag in test_data:\n","    pass\n","\n","print(\"The AUC is {}\")\n","print(\"Classification report:\\n \")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mm0a-kLygFtq","colab_type":"text"},"source":["\n","#### 3 - Evaluate how well your model behaves out of it's training domain (test it on one of the other given files)"]},{"cell_type":"code","metadata":{"id":"oQSGo1bKgGdx","colab_type":"code","outputId":"42ec5511-db82-4414-b381-14dac3d89ff5","executionInfo":{"status":"ok","timestamp":1583156052757,"user_tz":-60,"elapsed":18505,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["\n","out_of_domain1, no_match = get_data(\"./amazon_cells_labelled.csv\")\n","print(\"Got {} examples, and failed to capture \"\n","\"{} examples.\".format(len(out_of_domain1), no_match))\n","out_of_domain2, no_match = get_data(\"./yelp_labelled.csv\")\n","print(\"Got {} examples, and failed to capture \"\n","\"{} examples.\".format(len(out_of_domain1), no_match))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Got 1000 examples, and failed to capture 0 examples.\n","Got 1000 examples, and failed to capture 8 examples.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W1VzoO0MtQmF","colab_type":"code","outputId":"aa520628-a70b-4f55-b0eb-37f97b09d2d3","executionInfo":{"status":"ok","timestamp":1583156072754,"user_tz":-60,"elapsed":505,"user":{"displayName":"Ghazi Felhi","photoUrl":"","userId":"05726306300565644804"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["\n","ground_truth, scores = [], []\n","with torch.no_grad():\n","  for sentence, tag in out_of_domain1:\n","    pass\n","\n","print(\"The AUC for the first out of domain corpus is {}\")\n","print(\"Classification report:\\n \")\n","\n","ground_truth, scores = [], []\n","with torch.no_grad():\n","  for sentence, tag in out_of_domain2:\n","    pass\n","\n","print(\"The AUC for the second out of domain corpus is {}\")\n","print(\"Classification report:\\n \")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The AUC for the first out of domain corpus is {}\n","The AUC for the second out of domain corpus is {}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mVPLcfTetQFw","colab_type":"text"},"source":["### 4. Analysing token-wise tags\n","Go on and try to input some sentences of your own making, and to see how the score varies throughout the sentence."]},{"cell_type":"code","metadata":{"id":"ScyHqs1ct8Ea","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","\n","def sentiment_heatmap(sentence):\n","  tokens = [str(w.text) for w in tokenizer(sentence)]\n","  sentence_in = prepare_sequence(tokens, word_to_ix)\n","  tag_scores = model(sentence_in)\n","  token_sentiments = torch.exp(tag_scores)[:, :, 1].detach().numpy()\n","  sns.heatmap(token_sentiments, xticklabels=tokens)\n","  plt.title(\"sentence score: \"\n","  \"{}\".format(torch.mean(torch.exp(tag_scores)[:, :, 1]).item()))\n","  plt.show()\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TcpN8RV1vJj_","colab_type":"code","colab":{}},"source":["sample_sentences = [\n","                    \"Awful acting throughout the movie\",\n","                    \"I'd like to say that the actor was awful\",\n","                    \"Delightful scenery at the opening act of the movie\",\n","                    \"Bad movie, but pretty actress\"\n","]\n","for sentence in sample_sentences:\n","  sentiment_heatmap(sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAcuZCZOzEW2","colab_type":"text"},"source":["**Question**\n","- What would you say about the interpretability of the results ? to what can we blame for this ?\n","- Do you think averaging the token scores was a good decision ? How could we do better ?"]}]}